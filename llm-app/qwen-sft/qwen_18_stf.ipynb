{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72425c53",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 1.加载1.8B Chat Int4模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22a91a8a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\miniconda3\\envs\\python10\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "2024-07-03 15:22:51,057 - modelscope - INFO - PyTorch version 2.3.0 Found.\n",
      "2024-07-03 15:22:51,059 - modelscope - INFO - Loading ast index from C:\\Users\\Administrator\\.cache\\modelscope\\ast_indexer\n",
      "2024-07-03 15:22:51,119 - modelscope - INFO - Loading done! Current index file version is 1.15.0, with md5 c2bcd3e1333ef2723eeef346380e21f9 and a total number of 980 components indexed\n",
      "2024-07-03 15:22:54,840 - modelscope - WARNING - Model revision not specified, use revision: v1.0.0\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "Try importing flash-attention for faster inference...\n",
      "Warning: import flash_attn rotary fail, please install FlashAttention rotary to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/rotary\n",
      "Warning: import flash_attn rms_norm fail, please install FlashAttention layer_norm to get higher efficiency https://github.com/Dao-AILab/flash-attention/tree/main/csrc/layer_norm\n",
      "Warning: import flash_attn fail, please install FlashAttention to get higher efficiency https://github.com/Dao-AILab/flash-attention\n",
      "D:\\ProgramData\\miniconda3\\envs\\python10\\lib\\site-packages\\transformers\\modeling_utils.py:4565: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at E:/develop-workspace/model_repository/modelscope\\qwen\\Qwen-1_8B-Chat-Int4 were not used when initializing QWenLMHeadModel: ['transformer.h.0.attn.c_proj.bias', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.0.mlp.w1.bias', 'transformer.h.0.mlp.w2.bias', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.1.mlp.w1.bias', 'transformer.h.1.mlp.w2.bias', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.10.mlp.w1.bias', 'transformer.h.10.mlp.w2.bias', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.11.mlp.w1.bias', 'transformer.h.11.mlp.w2.bias', 'transformer.h.12.attn.c_proj.bias', 'transformer.h.12.mlp.c_proj.bias', 'transformer.h.12.mlp.w1.bias', 'transformer.h.12.mlp.w2.bias', 'transformer.h.13.attn.c_proj.bias', 'transformer.h.13.mlp.c_proj.bias', 'transformer.h.13.mlp.w1.bias', 'transformer.h.13.mlp.w2.bias', 'transformer.h.14.attn.c_proj.bias', 'transformer.h.14.mlp.c_proj.bias', 'transformer.h.14.mlp.w1.bias', 'transformer.h.14.mlp.w2.bias', 'transformer.h.15.attn.c_proj.bias', 'transformer.h.15.mlp.c_proj.bias', 'transformer.h.15.mlp.w1.bias', 'transformer.h.15.mlp.w2.bias', 'transformer.h.16.attn.c_proj.bias', 'transformer.h.16.mlp.c_proj.bias', 'transformer.h.16.mlp.w1.bias', 'transformer.h.16.mlp.w2.bias', 'transformer.h.17.attn.c_proj.bias', 'transformer.h.17.mlp.c_proj.bias', 'transformer.h.17.mlp.w1.bias', 'transformer.h.17.mlp.w2.bias', 'transformer.h.18.attn.c_proj.bias', 'transformer.h.18.mlp.c_proj.bias', 'transformer.h.18.mlp.w1.bias', 'transformer.h.18.mlp.w2.bias', 'transformer.h.19.attn.c_proj.bias', 'transformer.h.19.mlp.c_proj.bias', 'transformer.h.19.mlp.w1.bias', 'transformer.h.19.mlp.w2.bias', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.2.mlp.w1.bias', 'transformer.h.2.mlp.w2.bias', 'transformer.h.20.attn.c_proj.bias', 'transformer.h.20.mlp.c_proj.bias', 'transformer.h.20.mlp.w1.bias', 'transformer.h.20.mlp.w2.bias', 'transformer.h.21.attn.c_proj.bias', 'transformer.h.21.mlp.c_proj.bias', 'transformer.h.21.mlp.w1.bias', 'transformer.h.21.mlp.w2.bias', 'transformer.h.22.attn.c_proj.bias', 'transformer.h.22.mlp.c_proj.bias', 'transformer.h.22.mlp.w1.bias', 'transformer.h.22.mlp.w2.bias', 'transformer.h.23.attn.c_proj.bias', 'transformer.h.23.mlp.c_proj.bias', 'transformer.h.23.mlp.w1.bias', 'transformer.h.23.mlp.w2.bias', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.3.mlp.w1.bias', 'transformer.h.3.mlp.w2.bias', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.4.mlp.w1.bias', 'transformer.h.4.mlp.w2.bias', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.5.mlp.w1.bias', 'transformer.h.5.mlp.w2.bias', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.6.mlp.w1.bias', 'transformer.h.6.mlp.w2.bias', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.7.mlp.w1.bias', 'transformer.h.7.mlp.w2.bias', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.8.mlp.w1.bias', 'transformer.h.8.mlp.w2.bias', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.9.mlp.w1.bias', 'transformer.h.9.mlp.w2.bias']\n",
      "- This IS expected if you are initializing QWenLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing QWenLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from modelscope import snapshot_download\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "# cache_dir模型缓存目录自定义\n",
    "model_root_dir=\"E:/develop-workspace/model_repository/modelscope\"\n",
    "# cache_dir模型缓存目录自定义\n",
    "model_dir = snapshot_download('qwen/Qwen-1_8B-Chat-Int4',cache_dir=model_root_dir)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409fd61f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2.对话测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85ddd250",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "C:\\Users\\Administrator\\.cache\\huggingface\\modules\\transformers_modules\\Qwen-1_8B-Chat-Int4\\modeling_qwen.py:530: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = F.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！有什么我可以帮助你的吗？\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"你好\", history=None)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "745db571",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好呀！我是一只小猫，每天都很开心呢。你喜欢什么呢？\n"
     ]
    }
   ],
   "source": [
    "# Qwen-1.8B-Chat现在可以通过调整系统指令（System Prompt），实现角色扮演，语言风格迁移，任务设定，行为设定等能力。\n",
    "# Qwen-1.8B-Chat can realize roly playing, language style transfer, task setting, and behavior setting by system prompt.\n",
    "response, _ = model.chat(tokenizer, \"你好呀\", history=None, system=\"请用二次元可爱语气和我说话\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f20997ed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your colleague sounds very hardworking and dedicated, and I am sure their efforts have paid off. Their commitment to their job is truly commendable and it is inspiring to see someone work so diligently in their field. Well done! Keep up the great work.\n"
     ]
    }
   ],
   "source": [
    "response, _ = model.chat(tokenizer, \"My colleague works diligently\", history=None, system=\"You will write beautiful compliments according to needs\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c92a38",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 3.提示词工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4af419a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "很抱歉，我无法完成这项任务。这是一个文本输入问题，需要使用自然语言处理技术来解析和回答。在给出的问题中，没有提供足够的上下文或信息来确定哪种类型的文本应该回答这个问题，例如新闻报道、故事、指南、天气预报等。请提供更多详细的信息，我会尽力帮助你。\n",
      "[('\\n给定一句话：“青岛4月6日下雨么?”，请你按步骤要求工作。\\n\\n步骤1：识别这句话中的城市和日期共2个信息\\n步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\\n\\n请问，这个JSON字符串是：\\n', '很抱歉，我无法完成这项任务。这是一个文本输入问题，需要使用自然语言处理技术来解析和回答。在给出的问题中，没有提供足够的上下文或信息来确定哪种类型的文本应该回答这个问题，例如新闻报道、故事、指南、天气预报等。请提供更多详细的信息，我会尽力帮助你。')]\n"
     ]
    }
   ],
   "source": [
    "Q='青岛4月6日下雨么?'\n",
    "\n",
    "prompt_template='''\n",
    "给定一句话：“%s”，请你按步骤要求工作。\n",
    "\n",
    "步骤1：识别这句话中的城市和日期共2个信息\n",
    "步骤2：根据城市和日期信息，生成JSON字符串，格式为{\"city\":城市,\"date\":日期}\n",
    "\n",
    "请问，这个JSON字符串是：\n",
    "'''\n",
    "prompt=prompt_template%Q\n",
    "resp,hist=model.chat(tokenizer,prompt,history=None)\n",
    "print(resp)\n",
    "print(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3938e4df",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4.生成SFT微调数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac6bf41",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Qwen的SFT数据格式要求:\n",
    "```\n",
    "[\n",
    "  {\n",
    "    \"id\": \"identity_0\",\n",
    "    \"conversations\": [\n",
    "      {\n",
    "        \"from\": \"user\",\n",
    "        \"value\": \"你好\"\n",
    "      },\n",
    "      {\n",
    "        \"from\": \"assistant\",\n",
    "        \"value\": \"我是一个语言模型，我叫通义千问。\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0fa1cf20",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import time \n",
    "\n",
    "# 城市数据\n",
    "with open('city.txt','r',encoding='utf-8') as fp:\n",
    "    city_list=fp.readlines()\n",
    "    city_list=[line.strip().split(' ')[1] for line in city_list]\n",
    "\n",
    "\n",
    "Q_arr=[]\n",
    "A_arr=[]\n",
    "train_data=[]\n",
    "\n",
    "Q_list=[\n",
    "    ('{city}{year}年{month}月{day}日的天气','%Y-%m-%d'),\n",
    "    ('{city}{year}年{month}月{day}号的天气','%Y-%m-%d'),\n",
    "    ('{city}{month}月{day}日的天气','%m-%d'),\n",
    "    ('{city}{month}月{day}号的天气','%m-%d'),\n",
    "\n",
    "    ('{year}年{month}月{day}日{city}的天气','%Y-%m-%d'),\n",
    "    ('{year}年{month}月{day}号{city}的天气','%Y-%m-%d'),\n",
    "    ('{month}月{day}日{city}的天气','%m-%d'),\n",
    "    ('{month}月{day}号{city}的天气','%m-%d'),\n",
    "\n",
    "    ('你们{year}年{month}月{day}日去{city}玩吗？','%Y-%m-%d'),\n",
    "    ('你们{year}年{month}月{day}号去{city}玩么？','%Y-%m-%d'),\n",
    "    ('你们{month}月{day}日去{city}玩吗？','%m-%d'),\n",
    "    ('你们{month}月{day}号去{city}玩吗？','%m-%d'),\n",
    "]\n",
    "\n",
    "# 生成一批\"1月2号\"、\"1月2日\"、\"2023年1月2号\", \"2023年1月2日\", \"2023-02-02\", \"03-02\"之类的话术, 教会它做日期转换\n",
    "for i in range(1000):\n",
    "    Q=Q_list[random.randint(0,len(Q_list)-1)]\n",
    "    city=city_list[random.randint(0,len(city_list)-1)]\n",
    "    year=random.randint(1990,2025)\n",
    "    month=random.randint(1,12)\n",
    "    day=random.randint(1,28)\n",
    "    time_str='{}-{}-{}'.format(year,month,day)\n",
    "    date_field=time.strftime(Q[1],time.strptime(time_str,'%Y-%m-%d'))\n",
    "    Q=Q[0].format(city=city,year=year,month=month,day=day) # 问题\n",
    "    #A=json.dumps({'city':city,'date':date_field},ensure_ascii=False)  # 回答\n",
    "    A={'city':city,'date':date_field} # 回答\n",
    "\n",
    "    #Q_arr.append(prompt_template%(Q,))\n",
    "    #A_arr.append(A)\n",
    "    \n",
    "    example=  {\n",
    "        \"id\": \"identity_{}\".format(i),\n",
    "        \"conversations\": [\n",
    "          {\n",
    "            \"from\": \"user\",\n",
    "            \"value\": prompt_template%(Q,)\n",
    "          },\n",
    "          {\n",
    "            \"from\": \"assistant\",\n",
    "            \"value\": A\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    train_data.append(example)\n",
    "\n",
    "# 将数据写入 JSON 文件\n",
    "with open( 'train_data.json', 'w',encoding='utf-8') as fp:\n",
    "    fp.write(json.dumps(train_data,indent=4))\n",
    "print(\"样本数量：\",len(train_data))\n",
    "    \n",
    "\n",
    "#import pandas as pd \n",
    "#df=pd.DataFrame({'Prompt':Q_arr,'Completion':A_arr})\n",
    "#df.to_excel('train.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeddd15",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4.1下载Qwen仓库\n",
    "\n",
    "#!git clone https://gitcode.com/QwenLM/Qwen.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2f314e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4.2 Window进行微调\n",
    "\n",
    "调用Qwen/finetune.py文件进行配置与微调。\n",
    "\n",
    "\n",
    "- --model_name_or_path Qwen-1_8B-Chat：指定预训练模型的名称或路径，这里是使用名为\"Qwen-1_8B-Chat\"的预训练模型。\n",
    "--data_path chat.json：指定训练数据和验证数据的路径，这里是使用名为\"chat.json\"的文件。\n",
    "--fp16 True：指定是否使用半精度浮点数（float16）进行训练，这里设置为True。\n",
    "--output_dir output_qwen：指定输出目录，这里是将训练结果保存到名为\"output_qwen\"的文件夹中。\n",
    "--num_train_epochs 5：指定训练的轮数，这里是训练5轮。\n",
    "--per_device_train_batch_size 2：指定每个设备（如GPU）上用于训练的批次大小，这里是每个设备上训练2个样本。\n",
    "--per_device_eval_batch_size 1：指定每个设备上用于评估的批次大小，这里是每个设备上评估1个样本。\n",
    "--gradient_accumulation_steps 8：指定梯度累积步数，这里是梯度累积8步后再更新模型参数。\n",
    "--evaluation_strategy \"no\"：指定评估策略，这里是不进行评估。\n",
    "--save_strategy \"steps\"：指定保存策略，这里是每隔一定步数（如1000步）保存一次模型。\n",
    "--save_steps 1000：指定保存步数，这里是每隔1000步保存一次模型。\n",
    "--save_total_limit 10：指定最多保存的模型数量，这里是最多保存10个模型。\n",
    "--learning_rate 3e-4：指定学习率，这里是3e-4。\n",
    "--weight_decay 0.1：指定权重衰减系数，这里是0.1。\n",
    "--adam_beta2 0.95：指定Adam优化器的beta2参数，这里是0.95。\n",
    "--warmup_ratio 0.01：指定预热比例，这里是预热比例为总步数的1%。\n",
    "--lr_scheduler_type \"cosine\"：指定学习率调度器类型，这里是余弦退火调度器。\n",
    "--logging_steps 1：指定日志记录步数，这里是每1步记录一次日志。\n",
    "--report_to \"none\"：指定报告目标，这里是不报告任何信息。\n",
    "--model_max_length 512：指定模型的最大输入长度，这里是512个字符。\n",
    "--lazy_preprocess True：指定是否使用懒加载预处理，这里设置为True。\n",
    "--gradient_checkpointing：启用梯度检查点技术，可以在训练过程中节省显存并加速训练。\n",
    "--use_lora：指定是否使用LORA（Layer-wise Relevance Analysis）技术，这里设置为True\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "29606d9a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "!python ./Qwen/finetune.py \\\n",
    "--model_name_or_path \"E:/develop-workspace/model_repository/modelscope/qwen/Qwen-1_8B-Chat-Int4\" \\\n",
    "--data_path train_data.json \\\n",
    "--fp16 True \\\n",
    "--output_dir output_qwen \\\n",
    "--num_train_epochs 10 \\\n",
    "--per_device_train_batch_size 2 \\\n",
    "--per_device_eval_batch_size 1 \\\n",
    "--gradient_accumulation_steps 8 \\\n",
    "--evaluation_strategy \"no\" \\\n",
    "--save_strategy \"steps\" \\\n",
    "--save_steps 1000 \\\n",
    "--save_total_limit 10 \\\n",
    "--learning_rate 3e-4 \\\n",
    "--weight_decay 0.1 \\\n",
    "--adam_beta2 0.95 \\\n",
    "--warmup_ratio 0.01 \\\n",
    "--lr_scheduler_type \"cosine\" \\\n",
    "--logging_steps 1 \\\n",
    "--report_to \"none\" \\\n",
    "--model_max_length 512 \\\n",
    "--lazy_preprocess True \\\n",
    "--gradient_checkpointing True \\\n",
    "--use_lora True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527cd4af",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 4.3 Linux系统微调模型，生成到output_qwen\n",
    "bash finetune/finetune_qlora_single_gpu.sh  -m /root/.cache/modelscope/hub/qwen/Qwen-1_8B-Chat-Int4 -d /root/Qwen/train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4227424f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5. 只加载微调的后Lora模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fddac23",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    'output_qwen', # path to the output directory\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fc21b7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.generation_config.top_p=0 # 只选择概率最高的token\n",
    "\n",
    "Q_list=['2020年4月16号三亚下雨么？','青岛3-15号天气预报','5月6号下雪么，城市是威海','青岛2023年12月30号有雾霾么?','我打算6月1号去北京旅游，请问天气怎么样？','你们打算1月3号坐哪一趟航班去上海？','小明和小红是8月8号在上海结婚么?',\n",
    "        '一起去东北看冰雕么，大概是1月15号左右，我们3个人一起']\n",
    "for Q in Q_list:\n",
    "    prompt=prompt_template%(Q,)\n",
    "    A,hist=model.chat(tokenizer,prompt,history=None)\n",
    "    print('Q:%s\\nA:%s\\n'%(Q,A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa46f658",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prompt='青岛海边钓鱼需要特别注意什么？'\n",
    "resp,hist=model.chat(tokenizer,prompt,history=None)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba8d2d0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 6.模型合并\n",
    "与全参数微调不同，LoRA和Q-LoRA的训练只需存储adapter部分的参数。使用LoRA训练后的模型，可以选择先合并并存储模型（LoRA支持合并，Q-LoRA不支持），再用常规方式读取你的新模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da31ffe0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from peft import AutoPeftModelForCausalLM \n",
    "from transformers import AutoTokenizer \n",
    "\n",
    "\n",
    "# 分词\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"output_qwen\", trust_remote_code=True ) \n",
    "tokenizer.save_pretrained(\"qwen-1_8b-finetune\")\n",
    "\n",
    "# 模型\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\"output_qwen\", device_map=\"auto\", trust_remote_code=True ).eval() \n",
    "merged_model = model.merge_and_unload() \n",
    "merged_model.save_pretrained(\"qwen-1_8b-finetune\", max_shard_size=\"2048MB\", safe_serialization=True) # 最大分片2g\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df930a11",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 7.本地部署微调合并模型\n",
    "使用微调后且合并的模型进行本地部署。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e01abc0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig \n",
    "\n",
    "\n",
    "query = \"识别以下句子中的地址信息，并按照{address:['地址']}的格式返回。如果没有地址，返回{address:[]}。句子为：在一本关于人文的杂志中，我们发现了一篇介绍北京市海淀区科学院南路76号社区服务中心一层的文章，文章深入探讨了该地点的人文历史背景以及其对于当地居民的影响。\"\n",
    "local_model_path = \"qwen-1_8b-finetune\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(local_model_path, device_map=\"auto\", trust_remote_code=True).eval()\n",
    "response, history = model.chat(tokenizer, query, history=None)\n",
    "print(\"回答如下:\\n\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}