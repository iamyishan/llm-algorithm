{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前提说明\n",
    "\n",
    "- tranformer原版的decoder比较复杂，一般也不会让写。这里的 Decoder 一般指的是 CausalLM，具体变化是少了 encoder 部分的输入  \n",
    "所以也就没有了 encoder and decoder cross attention。\n",
    "- 因为重点希望写 CausalLM，所以没有 Cross attention 和 也省略了 token embedding 这一步"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 知识点\n",
    "- transformers decoder 的流程是：input -> self-attention -> cross-attention -> FFN\n",
    "- causalLM decoder 的流程是 input -> self-attention -> FFN\n",
    "- 其他 [self-attention, FFN] 是一个 block，一般会有很多的 block\n",
    "- FFN 矩阵有两次变化，一次升维度，一次降维度。其中 LLaMA 对于 GPT 的改进还有把 GeLU 变成了 SwishGLU，多了一个矩阵。所以一般升维会从 4h -> 4h * 2 / 3\n",
    "- 原版的 transformers 用 post-norm, 后面 gpt2, llama 系列用的是 pre-norm。其中 llama 系列一般用 RMSNorm 代替 GPT and transformers decoder 中的 LayerNorm。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数说明：\n",
    "- 这里按照 transformers 中的 decoder 来写，用 post_norm 的方式实现，主意有 残差链接  \n",
    "- eps 是为了防止溢出；其中 llama 系列的模型一般用的是 RMSnorm 以及 pre-norm（为了稳定性）  \n",
    "- RMSnorm 没有一个 recenter 的操作，而 layernorm 是让模型重新变成 均值为 0，方差为 1  \n",
    "- RMS 使用 w平方根均值进行归一化 $\\sqrt{\\frac{1}{n} \\sum_{1}^{n}{a_i^2} }$\n",
    "\n",
    "- pre-norm output： x + Sublayer(LayerNorm(x))\n",
    "- post-norm output：LayerNorm(x + Sublayer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask: torch.Size([3, 8, 4, 4])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5028, 0.4972, 0.0000, 0.0000],\n",
      "          [0.3360, 0.3285, 0.3355, 0.0000],\n",
      "          [0.2488, 0.2496, 0.2394, 0.2622]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4883, 0.5117, 0.0000, 0.0000],\n",
      "          [0.3393, 0.3591, 0.3016, 0.0000],\n",
      "          [0.2479, 0.2510, 0.2269, 0.2742]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4859, 0.5141, 0.0000, 0.0000],\n",
      "          [0.3144, 0.3228, 0.3628, 0.0000],\n",
      "          [0.2310, 0.2406, 0.2649, 0.2635]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5100, 0.4900, 0.0000, 0.0000],\n",
      "          [0.3185, 0.3162, 0.3653, 0.0000],\n",
      "          [0.2433, 0.2398, 0.2648, 0.2522]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4967, 0.5033, 0.0000, 0.0000],\n",
      "          [0.3351, 0.3534, 0.3114, 0.0000],\n",
      "          [0.2622, 0.2583, 0.2253, 0.2543]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4900, 0.5100, 0.0000, 0.0000],\n",
      "          [0.3416, 0.3388, 0.3196, 0.0000],\n",
      "          [0.2504, 0.2573, 0.2465, 0.2458]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5035, 0.4965, 0.0000, 0.0000],\n",
      "          [0.3237, 0.3371, 0.3393, 0.0000],\n",
      "          [0.2400, 0.2411, 0.2614, 0.2574]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4942, 0.5058, 0.0000, 0.0000],\n",
      "          [0.3292, 0.3372, 0.3336, 0.0000],\n",
      "          [0.2376, 0.2559, 0.2399, 0.2666]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5018, 0.4982, 0.0000, 0.0000],\n",
      "          [0.5054, 0.4946, 0.0000, 0.0000],\n",
      "          [0.5113, 0.4887, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4491, 0.5509, 0.0000, 0.0000],\n",
      "          [0.4235, 0.5765, 0.0000, 0.0000],\n",
      "          [0.4213, 0.5787, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5198, 0.4802, 0.0000, 0.0000],\n",
      "          [0.5079, 0.4921, 0.0000, 0.0000],\n",
      "          [0.5168, 0.4832, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4851, 0.5149, 0.0000, 0.0000],\n",
      "          [0.4840, 0.5160, 0.0000, 0.0000],\n",
      "          [0.4819, 0.5181, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5069, 0.4931, 0.0000, 0.0000],\n",
      "          [0.5226, 0.4774, 0.0000, 0.0000],\n",
      "          [0.5107, 0.4893, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4869, 0.5131, 0.0000, 0.0000],\n",
      "          [0.4824, 0.5176, 0.0000, 0.0000],\n",
      "          [0.4861, 0.5139, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5164, 0.4836, 0.0000, 0.0000],\n",
      "          [0.5020, 0.4980, 0.0000, 0.0000],\n",
      "          [0.5026, 0.4974, 0.0000, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4933, 0.5067, 0.0000, 0.0000],\n",
      "          [0.5044, 0.4956, 0.0000, 0.0000],\n",
      "          [0.5070, 0.4930, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5024, 0.4976, 0.0000, 0.0000],\n",
      "          [0.3343, 0.3403, 0.3254, 0.0000],\n",
      "          [0.3277, 0.3334, 0.3389, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4908, 0.5092, 0.0000, 0.0000],\n",
      "          [0.3388, 0.3470, 0.3143, 0.0000],\n",
      "          [0.3227, 0.3383, 0.3390, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4938, 0.5062, 0.0000, 0.0000],\n",
      "          [0.3172, 0.3332, 0.3496, 0.0000],\n",
      "          [0.3334, 0.3342, 0.3324, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4979, 0.5021, 0.0000, 0.0000],\n",
      "          [0.3469, 0.3460, 0.3072, 0.0000],\n",
      "          [0.3400, 0.3278, 0.3322, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.4914, 0.5086, 0.0000, 0.0000],\n",
      "          [0.3359, 0.3434, 0.3206, 0.0000],\n",
      "          [0.3267, 0.3558, 0.3176, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5303, 0.4697, 0.0000, 0.0000],\n",
      "          [0.3448, 0.3106, 0.3446, 0.0000],\n",
      "          [0.3412, 0.3132, 0.3456, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5057, 0.4943, 0.0000, 0.0000],\n",
      "          [0.3640, 0.3076, 0.3283, 0.0000],\n",
      "          [0.3418, 0.3205, 0.3377, 0.0000]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "          [0.5019, 0.4981, 0.0000, 0.0000],\n",
      "          [0.3503, 0.3346, 0.3151, 0.0000],\n",
      "          [0.3378, 0.3440, 0.3183, 0.0000]]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 64])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入相关需要的包\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# 写一个 Block\n",
    "class SimpleDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, nums_head, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nums_head = nums_head\n",
    "        self.head_dim = hidden_dim // nums_head\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # 这里按照 transformers 中的 decoder 来写，用 post_norm 的方式实现，主意有 残差链接\n",
    "        # eps 是为了防止溢出；其中 llama 系列的模型一般用的是 RMSnorm 以及 pre-norm（为了稳定性）\n",
    "        # RMSnorm 没有一个 recenter 的操作，而 layernorm 是让模型重新变成 均值为 0，方差为 1\n",
    "        # RMS 使用 w平方根均值进行归一化 $\\sqrt{\\frac{1}{n} \\sum_{1}^{n}{a_i^2} }$\n",
    "        self.layernorm_att = nn.LayerNorm(hidden_dim, eps=0.00001)\n",
    "\n",
    "        \n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.o_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.drop_att = nn.Dropout(0.1)\n",
    "\n",
    "        # for ffn 准备\n",
    "        self.layernorm_ffn = nn.LayerNorm(hidden_dim, eps=0.00001)\n",
    "\n",
    "        self.up_proj = nn.Linear(hidden_dim, hidden_dim * 4)\n",
    "        self.down_proj = nn.Linear(hidden_dim * 4, hidden_dim)\n",
    "\n",
    "        # self.layernorm_ffn = nn.LayerNorm(hidden_dim, eps=0.00001)\n",
    "        self.act_fn = nn.ReLU()\n",
    "        self.drop_ffn = nn.Dropout(0.1)\n",
    "\n",
    "    def attention_output(self, query,    key, value, attention_mask=None):\n",
    "        # 计算两者相关性\n",
    "        \n",
    "        key=key.transpose(2,3) # (batch_size,num_head,seq,head_dim) => (batch_size,num_head,head_dim,seq)\n",
    "        att_weight=torch.matmul(query,key) / math.sqrt(self.head_dim)\n",
    "        # attention mask 进行依次调整；变成 causal_attention\n",
    "        if attention_mask is not None:\n",
    "            # 变成下三角矩阵,例如\n",
    "            #tensor([[1, 0, 0],\n",
    "                    # [4, 5, 0],\n",
    "                    # [7, 8, 9]])\n",
    "            attention_mask=attention_mask.tril()\n",
    "            att_weight=att_weight.masked_fill(attention_mask==0,float(\"-1e20\"))\n",
    "        else:\n",
    "             # 人工构造一个下三角的 attention mask\n",
    "             attention_mask=torch.ones_like(att_weight).tril()\n",
    "\n",
    "        att_weight=torch.softmax(att_weight,dim=-1)\n",
    "        print(att_weight)\n",
    "        \n",
    "        att_weight=self.drop_att(att_weight)\n",
    "\n",
    "        mid_output=torch.matmul(att_weight,value)\n",
    "        # mid_output shape is: (batch, nums_head, seq, head_dim)\n",
    "        mid_output=mid_output.transpose(1,2).contiguous()\n",
    "        \n",
    "        batch,seq,_,_=mid_output.size()\n",
    "        mid_output=mid_output.view(batch,seq,-1)\n",
    "        output=self.o_proj(mid_output)\n",
    "        return output\n",
    "\n",
    "    def attention_block(self,X,attention_mask=None):\n",
    "        batch_size,seq,_=X.size()\n",
    "        query=self.q_proj(X).view(batch_size,seq,self.nums_head,-1).transpose(1,2)  #(batch_size,num_head,seq,head_dim)\n",
    "        key=self.k_proj(X).view(batch_size,seq,self.nums_head,-1).transpose(1,2)\n",
    "        value=self.v_proj(X).view(batch_size,seq,self.nums_head,-1).transpose(1,2)\n",
    "        output = self.attention_output(\n",
    "            query,\n",
    "            key,\n",
    "            value,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        return self.layernorm_att(X+output)\n",
    "\n",
    "    def ffn_block(self,X):\n",
    "        up=self.act_fn(self.up_proj(X))\n",
    "        down = self.down_proj(up)\n",
    "        # 执行 dropout\n",
    "        down=self.drop_ffn(down)\n",
    "        # 进行 norm 操作\n",
    "        return self.layernorm_ffn(X+down)\n",
    "\n",
    "\n",
    "    def forward(self, X, attention_mask=None):\n",
    "        # X 一般假设是已经经过 embedding 的输入， (batch, seq, hidden_dim)\n",
    "        # attention_mask 一般指的是 tokenizer 后返回的 mask 结果，表示哪些样本需要忽略\n",
    "        # shape 一般是： (batch, nums_head, seq)\n",
    "\n",
    "        att_output = self.attention_block(X, attention_mask=attention_mask)\n",
    "        ffn_output = self.ffn_block(att_output)\n",
    "        return ffn_output\n",
    "        \n",
    "x = torch.rand(3, 4, 64)\n",
    "net = SimpleDecoder(64, 8)\n",
    "mask = (\n",
    "    torch.tensor([[1, 1, 1, 1], [1, 1, 0, 0], [1, 1, 1, 0]])\n",
    "    .unsqueeze(1)\n",
    "    .unsqueeze(2)\n",
    "    .repeat(1, 8, 4, 1)\n",
    ")\n",
    "print(\"mask:\",mask.shape)\n",
    "net(x, mask).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
