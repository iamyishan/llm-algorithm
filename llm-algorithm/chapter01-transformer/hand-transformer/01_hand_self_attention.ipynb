{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-Attention的公式\n",
    "$$\\text{SelfAttention}(X) = \\text{softmax}\\left(\\frac{Q \\cdot K}{\\sqrt{d}}\\right) \\cdot V$$\n",
    "$Q = K = V = W \\times X$  ，其中Q K V 对应不同的矩阵 W\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 补充知识点\n",
    "1. matmul 和 @ 符号是一样的作用\n",
    "2. 为什么要除以$\\sqrt{d}$    a. 防止梯度消失 b. 为了让 QK 的内积分布保持和输入一样\n",
    "3. 爱因斯坦方程表达式用法：torch.einsum(\"bqd,bkd-> bqk\", X, X).shape\n",
    "4. X.repeat(1, 1, 3) 表示在不同的维度进行 repeat操作，也可以用 tensor.expand 操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一层：简化版\n",
    "- 直接对着公式实现  $\\text{SelfAttention}(X) = \\text{softmax}\\left(\\frac{Q \\cdot K}{\\sqrt{d}}\\right) \\cdot V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4714, 0.5286],\n",
      "         [0.4424, 0.5576]],\n",
      "\n",
      "        [[0.4997, 0.5003],\n",
      "         [0.4813, 0.5187]],\n",
      "\n",
      "        [[0.4918, 0.5082],\n",
      "         [0.4973, 0.5027]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0865, -0.5982,  0.1739,  0.2889],\n",
       "         [-0.0796, -0.5996,  0.1771,  0.2939]],\n",
       "\n",
       "        [[ 0.1381, -0.2920,  0.0974,  0.2454],\n",
       "         [ 0.1341, -0.2924,  0.0998,  0.2431]],\n",
       "\n",
       "        [[ 0.0957, -0.4515,  0.0607,  0.3143],\n",
       "         [ 0.0945, -0.4525,  0.0609,  0.3142]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttenionV1(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim=hidden_dim\n",
    "        # 一般 Linear 都是默认有 bias\n",
    "        # 一般来说， input dim 的 hidden dim\n",
    "        self.query_proj=nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.key_proj=nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.valye_proj=nn.Linear(hidden_dim,hidden_dim)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        # X shape is: (batch, seq_len, hidden_dim)， 一般是和 hidden_dim 相同\n",
    "        # 但是 X 的 final dim 可以和 hidden_dim 不同\n",
    "        Q=self.query_proj(X)\n",
    "        K=self.key_proj(X)\n",
    "        V=self.valye_proj(X)\n",
    "        # shape is: (batch, seq_len, seq_len)\n",
    "        # torch.matmul 可以改成 Q @ K.T\n",
    "        # 其中 K 需要改成 shape 为： (batch, hidden_dim, seq_len)\n",
    "        attention_value=torch.matmul(Q,K.transpose(-1,-2))\n",
    "        attention_weight=torch.softmax(attention_value/math.sqrt(self.hidden_dim),dim=-1)\n",
    "\n",
    "        print(attention_weight)\n",
    "        # shape is: (batch, seq_len, hidden_dim)\n",
    "        output=torch.matmul(attention_weight,V)\n",
    "        return output\n",
    "\n",
    "X=torch.rand(3,2,4)\n",
    "\n",
    "net=SelfAttenionV1(X.shape[-1])\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二层: 效率优化\n",
    "- 上面那哪些操作可以合并矩阵优化呢？- QKV 矩阵计算的时候，可以合并成一个大矩阵计算  \n",
    "但是当前 transformers 实现中，其实是三个不同的 Linear 层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 4])\n",
      "torch.Size([3, 4, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5421, -1.0168, -0.3451, -0.4420],\n",
       "         [-0.5412, -1.0166, -0.3441, -0.4415]],\n",
       "\n",
       "        [[-0.5084, -0.9868, -0.3949, -0.4141],\n",
       "         [-0.5059, -0.9880, -0.3945, -0.4135]],\n",
       "\n",
       "        [[-0.4093, -0.9306, -0.1960, -0.3420],\n",
       "         [-0.4085, -0.9300, -0.1953, -0.3413]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SelfAttenionV2(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim=hidden_dim\n",
    "        # 在维度较小情况下可以合并计算，但是Llama, qwen, gpt的参数很大，还是分开计算\n",
    "        self.proj=nn.Linear(hidden_dim,hidden_dim*3)\n",
    "    def forward(self,X):\n",
    "        # X shape is: (batch, seq, dim)\n",
    "        QKV=self.proj(X) # (batch, seq, dim * 3)\n",
    "        # reshape 从希望的 q, k, v的形式\n",
    "        Q,K,V=torch.split(QKV,self.hidden_dim,dim=-1)\n",
    "        print(Q.shape)\n",
    "        print(K.transpose(-1,-2).shape)\n",
    "        att_weight=torch.softmax((Q @ K.transpose(-1,-2))/math.sqrt(self.hidden_dim),dim=-1)\n",
    "        output=att_weight @ V\n",
    "        return output\n",
    "         \n",
    "X=torch.rand(3,2,4)\n",
    "net=SelfAttenionV2(X.shape[-1])\n",
    "net(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第三重: 加入细节\n",
    "- 看上去 self attention 实现很简单，但里面还有一些细节，还有哪些细节呢？\n",
    "- attention 计算的时候有 dropout，而且是比较奇怪的位置\n",
    "- attention 计算的时候一般会加入 attention_mask，因为样本会进行一些 padding 操作；\n",
    "- MultiHeadAttention 过程中，除了 QKV 三个矩阵之外，还有一个 output 对应的投影矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "att_weight: torch.Size([3, 4, 4])\n",
      "attention_mask: torch.Size([3, 4, 4])\n",
      "torch.Size([3, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class SelfAttentionV3(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.proj = nn.Linear(hidden_dim, hidden_dim * 3)\n",
    "        self.att_drop = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, X, attention_mask=None):\n",
    "        # X shape: (batch, seq, dim)\n",
    "        QKV = self.proj(X)  # (batch, seq, dim * 3)\n",
    "        Q, K, V = torch.split(QKV, self.hidden_dim, dim=-1)  # 每个形状: (batch, seq, dim)\n",
    "\n",
    "        # 计算注意力权重\n",
    "        att_weight = Q @ K.transpose(-1, -2) / math.sqrt(self.hidden_dim)  # (batch, seq, seq)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # 确保 attention_mask 是布尔张量\n",
    "            attention_mask = attention_mask == 0  # 将 0 转换为 True，其他值转换为 False\n",
    "            # 使用 masked_fill 填充极小值\n",
    "            print(\"att_weight:\",att_weight.shape)\n",
    "            print(\"attention_mask:\",attention_mask.shape)\n",
    "\n",
    "            att_weight = att_weight.masked_fill(attention_mask, float(\"-1e20\"))\n",
    "\n",
    "        # 计算 softmax\n",
    "        att_weight = torch.softmax(att_weight, dim=-1)  # (batch, seq, seq)\n",
    "        att_weight = self.att_drop(att_weight)\n",
    "\n",
    "        # 计算输出\n",
    "        output = att_weight @ V  # (batch, seq, dim)\n",
    "        return output\n",
    "\n",
    "# 测试数据\n",
    "X = torch.rand(3, 4, 2)  # (batch_size=3, seq_len=4, hidden_dim=2)\n",
    "b = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 0],\n",
    "        [1, 1, 0, 0],\n",
    "        [1, 0, 0, 0],\n",
    "    ]\n",
    ")  # (batch_size=3, seq_len=4)\n",
    "mask = b.unsqueeze(dim=1).repeat(1, 4, 1)  # (batch_size=3, seq_len=4, seq_len=4)\n",
    "\n",
    "# 初始化网络\n",
    "net = SelfAttentionV3(X.shape[-1])\n",
    "output = net(X, mask)\n",
    "print(output.shape)  # 应该输出: torch.Size([3, 4, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第四层：完整写法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask: torch.Size([3, 4, 4])\n",
      "output: torch.Size([3, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# 定义常量\n",
    "DROPOUT_PROB = 0.1\n",
    "\n",
    "class SelfAttentionV4(nn.Module):\n",
    "    def __init__(self, dim) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "        # 定义线性变换层\n",
    "        self.query_proj = nn.Linear(dim, dim)\n",
    "        self.key_proj = nn.Linear(dim, dim)\n",
    "        self.value_proj = nn.Linear(dim, dim)\n",
    "        \n",
    "        # 定义 dropout 层\n",
    "        self.attention_dropout = nn.Dropout(DROPOUT_PROB)\n",
    "\n",
    "\n",
    "    def forward(self, X, attention_mask=None):\n",
    "        \"\"\"\n",
    "        X: 输入张量，形状为 (batch_size, seq_len, dim)\n",
    "        attention_mask: 注意力掩码，形状为 (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        # 计算 Q, K, V\n",
    "        Q = self.query_proj(X)\n",
    "        K = self.key_proj(X)\n",
    "        V = self.value_proj(X)\n",
    "\n",
    "        # 计算注意力权重\n",
    "        att_weight = Q @ K.transpose(-1, -2) / math.sqrt(self.dim)\n",
    "        \n",
    "        # 应用注意力掩码\n",
    "        if attention_mask is not None:\n",
    "            # 给 masked 位置填充一个极小的值-1e20，然后取exp指数函数负无穷就变为0\n",
    "            att_weight = att_weight.masked_fill(attention_mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        # 计算 softmax\n",
    "        att_weight = torch.softmax(att_weight, dim=-1)\n",
    "\n",
    "        # 应用 dropout\n",
    "        att_weight = self.attention_dropout(att_weight)\n",
    "\n",
    "        # 计算加权和\n",
    "        output = att_weight @ V\n",
    "        return output\n",
    "\n",
    "X = torch.rand(3, 4, 2)\n",
    "b = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 1, 0],\n",
    "        [1, 1, 0, 0],\n",
    "        [1, 0, 0, 0],\n",
    "    ]\n",
    ")\n",
    "mask = b.unsqueeze(dim=1).repeat(1, 4, 1)\n",
    "print(\"mask:\",mask.shape)\n",
    "net = SelfAttentionV4(X.shape[-1])\n",
    "output=net(X, mask)\n",
    "print(\"output:\",output.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
